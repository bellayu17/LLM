{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeVLEWLlKxr7",
        "outputId": "08d81f6a-3048-4a3a-c563-cd6c86310e60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "downloaded = drive.CreateFile({'id': \"File ID here\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "  return nltk.word_tokenize(sentence)\n",
        "\n",
        "def stem(word):\n",
        "  return stemmer.stem(word.lower())"
      ],
      "metadata": {
        "id": "jfIPJkVeL92T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(tokenized_setence, words):\n",
        "  sentence_words = [stem(word) for word in tokenized_setence]\n",
        "  bag = np.zeros(len(words), dtype=np.float32)\n",
        "  for idx, w in enumerate(words):\n",
        "    if w in sentence_words:\n",
        "      bag[idx] = 1\n",
        "  return bag"
      ],
      "metadata": {
        "id": "jtfM3Y5nMfJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "if Path(\"LLM.json\").is_file():\n",
        "  print(\"Already\")\n",
        "else:\n",
        "  print(\"Downloading\")\n",
        "  request = requests.get(\"https://raw.githubusercontent.com/bellayu17/LLM/main/LLM.json\")\n",
        "  with open(\"LLM.json\", \"wb\") as f:\n",
        "    f.write(request.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kr2Q3jqSSSTW",
        "outputId": "89d94502-e8ab-406a-9fd4-b4be33027ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('LLM.json', 'r') as f:\n",
        "  intents = json.load(f)"
      ],
      "metadata": {
        "id": "QIZHp2_BM_MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_words=[]\n",
        "tags=[]\n",
        "xy=[]\n",
        "for intent in intents[\"intents\"]:\n",
        "  tag = intent['tag']\n",
        "  tags.append(tag)\n",
        "\n",
        "  for pattern in intent['patterns']:\n",
        "    w = tokenize(pattern)\n",
        "    all_words.extend(w)\n",
        "    xy.append((w,tag))"
      ],
      "metadata": {
        "id": "NEkeO6ACSpP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the keywords. Let the model only take keywords and make response.\n",
        "ignore_words = ['?','.',',','-']\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))"
      ],
      "metadata": {
        "id": "sAm3ZYELTGFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=[]\n",
        "y_train=[]\n",
        "\n",
        "for (pattern_sentence, tag) in xy:\n",
        "  bag = bag_of_words(pattern_sentence, all_words)\n",
        "  X_train.append(bag)\n",
        "  label = tags.index(tag)\n",
        "  y_train.append(label)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "id": "ccUSbEkxTx_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build model"
      ],
      "metadata": {
        "id": "o4Co2nxxU4UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,\n",
        "               input_size,\n",
        "               hidden_size,\n",
        "               num_classes):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.l1 = nn.Linear(input_size, hidden_size)\n",
        "    self.l2 = nn.Linear(hidden_size, hidden_size)\n",
        "    self.l3 = nn.Linear(hidden_size, num_classes)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, X):\n",
        "    X = self.l1(X)\n",
        "    X = self.relu(X)\n",
        "    X = self.l2(X)\n",
        "    X = self.relu(X)\n",
        "    X = self.l3(X)\n",
        "    return X"
      ],
      "metadata": {
        "id": "705QbpH4Tx2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    self.n_samples = len(X_train)\n",
        "    self.X_data = X_train\n",
        "    self.y_data = y_train\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.X_data[index], self.y_data[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples"
      ],
      "metadata": {
        "id": "1pDKonpnTxRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup hyperparameters for specifying the model\n",
        "NUM_EPOCHS = 1000\n",
        "BATCH_SIZE = 8\n",
        "LEARNING_RATE = 0.1\n",
        "INPUT_SIZE = len(X_train[0])\n",
        "HIDDEN_SIZE = 8\n",
        "OUTPUT_SIZE = len(tags)"
      ],
      "metadata": {
        "id": "AU4NR69TWNCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ChatDataset()\n",
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=BATCH_SIZE,\n",
        "                          shuffle=True,\n",
        "                          num_workers=0)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = NeuralNet(input_size=INPUT_SIZE,\n",
        "                  hidden_size=HIDDEN_SIZE,\n",
        "                  num_classes=OUTPUT_SIZE).to(device)"
      ],
      "metadata": {
        "id": "XuhsonvSW5i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(),\n",
        "                            lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "p2anYwphX9YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build training and testing loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  for (words, labels) in train_loader:\n",
        "    words = words.to(device)\n",
        "    labels = labels.to(dtype=torch.long).to(device)\n",
        "\n",
        "    # Do the Forward Pass\n",
        "    labels_pred = model(words)\n",
        "\n",
        "    # Calculate the loss\n",
        "    loss = loss_fn(labels_pred, labels)\n",
        "\n",
        "    # Optimizer zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Loss backward\n",
        "    loss.backward()\n",
        "\n",
        "    # Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % 100 == 0:\n",
        "    print(f\"Epoch: {epoch} | Train loss: {loss:.5f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee_NorgyYPr2",
        "outputId": "43ce8cf4-0ab1-4ada-b2da-d855058aa393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train loss: 1.78206\n",
            "Epoch: 100 | Train loss: 0.22446\n",
            "Epoch: 200 | Train loss: 0.04590\n",
            "Epoch: 300 | Train loss: 0.01136\n",
            "Epoch: 400 | Train loss: 0.00322\n",
            "Epoch: 500 | Train loss: 0.00430\n",
            "Epoch: 600 | Train loss: 0.00296\n",
            "Epoch: 700 | Train loss: 0.00137\n",
            "Epoch: 800 | Train loss: 0.00123\n",
            "Epoch: 900 | Train loss: 0.00037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chatbot"
      ],
      "metadata": {
        "id": "Q4EC56iJZkcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bot_name = \"BabyBlues\"\n",
        "print(\"Let's Chat!\")\n",
        "while True:\n",
        "  sentence = input(\"You: \")\n",
        "  if sentence == \"quit\":\n",
        "    break\n",
        "\n",
        "  sentence = tokenize(sentence)\n",
        "  X = bag_of_words(sentence, all_words)\n",
        "  X = X.reshape(1, X.shape[0])\n",
        "  X = torch.from_numpy(X).to(device)\n",
        "\n",
        "  labels_pred = model(X)\n",
        "  __, predicted = torch.max(labels_pred, dim=1)\n",
        "\n",
        "  tag = tags[predicted.item()]\n",
        "\n",
        "  probs = torch.softmax(labels_pred, dim=1)\n",
        "  prob = probs[0][predicted.item()]\n",
        "  if prob.item() > 0.75:\n",
        "    for intent in intents['intents']:\n",
        "      if tag == intent['tag']:\n",
        "        print(f\"{bot_name}: {random.choice(intent['responses'])}\")\n",
        "  else:\n",
        "        print(f\"{bot_name}: Could you elaborate more?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "68G7nZFLZkEu",
        "outputId": "a6c572f7-525f-4212-c322-4da92dfdafae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Let's Chat!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-1f39fda6f97c>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Let's Chat!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    }
  ]
}